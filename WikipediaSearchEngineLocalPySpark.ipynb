{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e124c654-4b2a-4503-b9b4-21ff3157552e"
   },
   "source": [
    "# Setup\n",
    "Preliminary steps for installing packages, importing libraries, starting the Spark session, and loading the file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ACRmLyES8suG"
   },
   "source": [
    "## PySpark Session\n",
    "Initialize the PySpark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ebfe639a-8737-4b1a-85f6-4aaa26e5abc2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\brian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.4.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\brian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Requirement already satisfied: findspark in c:\\users\\brian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\brian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (2.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\brian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\brian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\brian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\brian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas) (1.25.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\brian\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install pyspark\n",
    "!pip install findspark\n",
    "!pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "36c68184-8f40-471d-9892-79b166ed122f"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pyspark\n",
    "import re\n",
    "import findspark\n",
    "import time\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as f\n",
    "import glob\n",
    "import os\n",
    "from pyspark.sql.types import StructType,StructField, StringType\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import max\n",
    "from collections import Counter\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "6HvMqRgCoJ9a"
   },
   "outputs": [],
   "source": [
    "# Function for formatting a time string\n",
    "def hms_string(sec_elapsed):\n",
    "    h = int(sec_elapsed / (60 * 60))\n",
    "    m = int((sec_elapsed % (60 * 60)) / 60)\n",
    "    s = sec_elapsed % 60\n",
    "    return \"{}:{:>02}:{:>05.2f}\".format(h, m, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "27fba0f3-c3da-4ca2-8a37-ead879280c3a"
   },
   "outputs": [],
   "source": [
    "# Create a spark context class\n",
    "#sc = SparkContext()\n",
    "\n",
    "# Other attempts at configuration changes\n",
    "#     .config(\"spark.sql.execution.arrow.pyspark.enabled\",\"true\") \\\n",
    "#     .config(\"spark.executor.heartbeatInterval\",\"3600\") \\\n",
    "#     .config(\"spark.sql.autoBroadcastJoinThreshold\",\"-1\") \\\n",
    "#     .config(\"spark.executor.memory\",'8g') \\\n",
    "\n",
    "# Create a spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local[*]') \\\n",
    "    .appName(\"Wikipedia Search Engine\") \\\n",
    "    .config(\"spark.driver.maxResultSize\",'0') \\\n",
    "    .config(\"spark.ui.showConsoleProgress\",\"true\") \\\n",
    "    .config(\"spark.driver.memory\",'12g') \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "3161e22a-b0a3-4dea-af24-e77b36d53cc8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://BRIAN-LAPTOP:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.4.1</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Wikipedia Search Engine</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x1b4bf89d750>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6f1d8fc3-9555-486e-bf79-34b390375ecc"
   },
   "source": [
    "## Load Data\n",
    "\n",
    "Load Wikipedia data (pre-processed) from local storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "37fbfc6c-721d-425a-b0f2-95f9036bafb9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:02.27\n"
     ]
    }
   ],
   "source": [
    "# Read JSON file and drop any bad records (DROPMALFORMED)\n",
    "# Select from various files, sorted from smallest to largest size\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# First 100 Wikipedia articles\n",
    "#main_df = spark.read.option(\"mode\", \"DROPMALFORMED\").json(\"C:\\\\Users\\\\Brian\\\\Downloads\\\\articlesSampleData.json\")\n",
    "\n",
    "# First 100000 Wikipedia articles\n",
    "main_df = spark.read.option(\"mode\", \"DROPMALFORMED\").json(\"C:\\\\Users\\\\Brian\\\\Downloads\\\\articles100000.json\")\n",
    "\n",
    "# First 500000 Wikipedia articles\n",
    "#main_df = spark.read.option(\"mode\", \"DROPMALFORMED\").json(\"C:\\\\Users\\\\Brian\\\\Downloads\\\\articles500000.json\")\n",
    "\n",
    "# First 1000000 Wikipedia articles\n",
    "#main_df = spark.read.option(\"mode\", \"DROPMALFORMED\").json(\"C:\\\\Users\\\\Brian\\\\Downloads\\\\articlesMillion.json\")\n",
    "\n",
    "# Half the Wikipedia JSON\n",
    "#main_df = spark.read.option(\"mode\", \"DROPMALFORMED\").json(\"C:\\\\Users\\\\Brian\\\\Downloads\\\\articlesHalf.json\")\n",
    "\n",
    "# Full Wikipedia JSON (Takes about a minute to run)\n",
    "#main_df = spark.read.option(\"mode\", \"DROPMALFORMED\").json(\"C:\\\\Users\\\\Brian\\\\Downloads\\\\articles.json\")\n",
    "\n",
    "# Further clean up bad records (found through testing that both cleanups are necessary)\n",
    "main_df = main_df.na.drop()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "28c24781-59f2-4a7c-a2e0-2cc87256b561"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'text', 'title']\n",
      "root\n",
      " |-- id: long (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      "\n",
      "+---+--------------------+--------------------+\n",
      "| id|                text|               title|\n",
      "+---+--------------------+--------------------+\n",
      "| 12|  short descripti...|           Anarchism|\n",
      "| 39|  Short descripti...|              Albedo|\n",
      "|290|  Short descripti...|                   A|\n",
      "|303|  Short descripti...|             Alabama|\n",
      "|305|  short descripti...|            Achilles|\n",
      "|307|  Short descripti...|     Abraham Lincoln|\n",
      "|308|  Short descripti...|           Aristotle|\n",
      "|309|  short descripti...|An American in Paris|\n",
      "|316|  Short descripti...|Academy Award for...|\n",
      "|324|  short descripti...|      Academy Awards|\n",
      "|330|  Use dmy dates d...|             Actrius|\n",
      "|332|  Short descripti...|     Animalia (book)|\n",
      "|334|  Short descripti...|International Ato...|\n",
      "|336|  short descripti...|            Altruism|\n",
      "|339|  Short descripti...|            Ayn Rand|\n",
      "|340|  short descripti...|        Alain Connes|\n",
      "|344|  Short descripti...|          Allan Dwan|\n",
      "|358|  short descripti...|             Algeria|\n",
      "|359|  Short descripti...|List of Atlas Shr...|\n",
      "|569|  Short descripti...|        Anthropology|\n",
      "|572|  short descripti...|Agricultural science|\n",
      "|573|  Short descripti...|             Alchemy|\n",
      "|579|  pp small yes   ...|               Alien|\n",
      "|580|  short descripti...|          Astronomer|\n",
      "|586|  Short descripti...|               ASCII|\n",
      "|590|  wiktionary Aust...|Austin (disambigu...|\n",
      "|593|  Short descripti...|           Animation|\n",
      "|594|  short descripti...|              Apollo|\n",
      "|595|  Short descripti...|        Andre Agassi|\n",
      "|597|  Distinguish Aus...|Austroasiatic lan...|\n",
      "|599|  short descripti...|Afroasiatic langu...|\n",
      "|600|  short descripti...|             Andorra|\n",
      "|612|  Short descripti...|     Arithmetic mean|\n",
      "|615|  short descripti...|American Football...|\n",
      "|617| redirect   Al Go...|         Albert Gore|\n",
      "|620|  Short descripti...|         Animal Farm|\n",
      "|621|  Short descripti...|           Amphibian|\n",
      "|624|  Short descripti...|              Alaska|\n",
      "|627|  short descripti...|         Agriculture|\n",
      "|628|  short descripti...|       Aldous Huxley|\n",
      "|630|  Wiktionary Ada ...|                 Ada|\n",
      "|632|  wiktionary Aber...|Aberdeen (disambi...|\n",
      "|633|  Short descripti...|               Algae|\n",
      "|634|  short descripti...|Analysis of variance|\n",
      "|639|  Short descripti...|              Alkane|\n",
      "|640|  short descripti...|Appellate procedu...|\n",
      "|642|  other uses Answ...|        Answer (law)|\n",
      "|643|  short descripti...|     Appellate court|\n",
      "|649|  Short descripti...|         Arraignment|\n",
      "|651|  short descripti...|America the Beaut...|\n",
      "|653|  short descripti...|Assistive technology|\n",
      "|655|  short descripti...|              Abacus|\n",
      "|656|  About acids in ...|                Acid|\n",
      "|657|  pp small yes   ...|             Bitumen|\n",
      "|659|  Short descripti...|American National...|\n",
      "|661|  Wiktionary argu...|Argument (disambi...|\n",
      "|662|  Short descripti...|           Apollo 11|\n",
      "|663|  Short descripti...|            Apollo 8|\n",
      "|664|  short descripti...|           Astronaut|\n",
      "|665|  Short descripti...|   A Modest Proposal|\n",
      "|666|  short descripti...|        Alkali metal|\n",
      "|670|  Short descripti...|            Alphabet|\n",
      "|673|  short descripti...|       Atomic number|\n",
      "|674|  short descripti...|             Anatomy|\n",
      "|675|  Short descripti...|Affirming the con...|\n",
      "|676|  Short descripti...|    Andrei Tarkovsky|\n",
      "|677|  Short descripti...|           Ambiguity|\n",
      "|678|  about the bibli...|                Abel|\n",
      "|679|  wiktionary anim...|Animal (disambigu...|\n",
      "|680|  Short descripti...|            Aardvark|\n",
      "|681|  Short descripti...|            Aardwolf|\n",
      "|682|  Short descripti...|               Adobe|\n",
      "|683|  short descripti...|           Adventure|\n",
      "|689|  Short descripti...|                Asia|\n",
      "|690|  Short descripti...|               Aruba|\n",
      "|691|  Short descripti...|Articles of Confe...|\n",
      "|694|  Wiktionary Asia...|Asia Minor (disam...|\n",
      "|698|  Short descripti...|      Atlantic Ocean|\n",
      "|700|  Short descripti...| Arthur Schopenhauer|\n",
      "|701|  Short descripti...|              Angola|\n",
      "|704|  Use mdy dates d...|Demographics of A...|\n",
      "|705|  Short descripti...|  Politics of Angola|\n",
      "|706|  short descripti...|   Economy of Angola|\n",
      "|708|  Short descripti...| Transport in Angola|\n",
      "|709|  Short descripti...|Angolan Armed Forces|\n",
      "|710|  Use dmy dates d...|Foreign relations...|\n",
      "|711|  short descripti...|Albert Sidney Joh...|\n",
      "|713|  Short descripti...|     Android (robot)|\n",
      "|717|  short descripti...|             Alberta|\n",
      "|728|  Short descripti...|List of anthropol...|\n",
      "|734|  Short descripti...|      Actinopterygii|\n",
      "|736|  Short descripti...|     Albert Einstein|\n",
      "|737|  Short descripti...|         Afghanistan|\n",
      "|738|  Short descripti...|             Albania|\n",
      "|740|  Short descripti...|               Allah|\n",
      "|742|  Use dmy dates d...|Algorithms (journal)|\n",
      "|746|  short descripti...|          Azerbaijan|\n",
      "|748|  Short descripti...|   Amateur astronomy|\n",
      "|751|  short descripti...|              Aikido|\n",
      "|752|  short descripti...|                 Art|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 100 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the dataframe as well as the data schema\n",
    "\n",
    "print(main_df.columns)\n",
    "main_df.printSchema()\n",
    "main_df.show(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "s9pzc3yYrTu5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get number of partitions\n",
    "main_df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJ3Yxuk-WrPg"
   },
   "source": [
    "## Cleanup\n",
    "\n",
    "Remove special characters and change all words to lowercase\n",
    "Do this at the very beginning so time isn't wasted doing it more than once in other steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "f8978073-0f64-4d49-8b98-dc39b67d1bca"
   },
   "outputs": [],
   "source": [
    "# Define function cleaning up text in each article\n",
    "\n",
    "def textCleanup(text):\n",
    "  # Only leave words and whitespace in string; convert all words to lowercase\n",
    "  return(re.sub(r'[^\\w\\s]', '', text).lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "HkavK-hXas15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+\n",
      "| id|                text|               title|\n",
      "+---+--------------------+--------------------+\n",
      "| 12|  short descripti...|           Anarchism|\n",
      "| 39|  short descripti...|              Albedo|\n",
      "|290|  short descripti...|                   A|\n",
      "|303|  short descripti...|             Alabama|\n",
      "|305|  short descripti...|            Achilles|\n",
      "|307|  short descripti...|     Abraham Lincoln|\n",
      "|308|  short descripti...|           Aristotle|\n",
      "|309|  short descripti...|An American in Paris|\n",
      "|316|  short descripti...|Academy Award for...|\n",
      "|324|  short descripti...|      Academy Awards|\n",
      "|330|  use dmy dates d...|             Actrius|\n",
      "|332|  short descripti...|     Animalia (book)|\n",
      "|334|  short descripti...|International Ato...|\n",
      "|336|  short descripti...|            Altruism|\n",
      "|339|  short descripti...|            Ayn Rand|\n",
      "|340|  short descripti...|        Alain Connes|\n",
      "|344|  short descripti...|          Allan Dwan|\n",
      "|358|  short descripti...|             Algeria|\n",
      "|359|  short descripti...|List of Atlas Shr...|\n",
      "|569|  short descripti...|        Anthropology|\n",
      "+---+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Elapsed time: 0:00:00.65\n"
     ]
    }
   ],
   "source": [
    "# Update text row in place; remove special characters and make all characters lowercase\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "main_df = main_df.withColumn(\"text\", pyspark.sql.functions.udf(lambda x: textCleanup(x))(\"text\"))\n",
    "main_df.show()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Z90CoejHMxF"
   },
   "source": [
    "# Indexing Engine\n",
    "Process each document in the text corpus to prepare them for text queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oUKvm-NpuSMZ"
   },
   "source": [
    "## Word Enumerator / IDF Count\n",
    "Build vocabulary of unique words and how many documents they appear in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "D8i2RDFfuRaa"
   },
   "outputs": [],
   "source": [
    "# Create column of unique words using a set (returned converted to string)\n",
    "main_df = main_df.withColumn(\"words\", pyspark.sql.functions.udf(lambda x: textCleanup(str(set(x.split()))))(\"text\"))\n",
    "\n",
    "# Working Method: Count how many documents each word appears in; this is probably where we can speed it up further\n",
    "#https://stackoverflow.com/questions/48927271/count-number-of-words-in-a-spark-dataframe\n",
    "vocabulary_df = main_df.withColumn('word', f.explode(f.split(f.col('words'), ' '))).groupBy('word').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3BzyAf962HkN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.27\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Processing spends most of its time here but that's because the previous line is lazily evaluated\n",
    "#https://stackoverflow.com/questions/41206255/convert-pyspark-sql-dataframe-dataframe-type-dataframe-to-dictionary\n",
    "word_counts = {row.asDict()['word'] : row.asDict()['count'] for row in vocabulary_df.collect()}\n",
    "\n",
    "# Output time taken\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "mr_FrDiRFQzf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlQTy0mzxrYv",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Alternative attempts at the Word Enumerator / IDF Count step\n",
    "These methods were either slower or didn't end up working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YV98biYfvFPS"
   },
   "outputs": [],
   "source": [
    "# Method 1: Works but doesn't work on large datasets (times out)\n",
    "# Convert to a set so we get only unique words in each string\n",
    "# This way we can count the inverse document frequency (IDF) of each term instead of the total number of occurrences\n",
    "#word_counts = text_df.flatMap(lambda x: set(x[0].split())).countByValue()\n",
    "#word_counts = main_df.select(col(\"wordCounts\")).rdd.flatMap(lambda x: dict(x)).countByValue()\n",
    "#word_counts = main_df.select(col(\"text\")).rdd.flatMap(lambda text: (text, 1)).reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Test printing out word_counts\n",
    "#print(sorted(word_counts.items()))\n",
    "#for i, (word, count) in enumerate(word_counts.items()):\n",
    "  #if i > 1000:\n",
    "    #break\n",
    "  #print(word, count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Method 2: Run after the term frequency / word counts step\n",
    "# The lambda function did not work with global variables\n",
    "# Declare global IDF dictionary\n",
    "#word_counts = dict()\n",
    "#list(word_counts.keys())\n",
    "\n",
    "# Use the already calculated word counts to count the words per document\n",
    "#def wordEnumeration(wordDictionary):\n",
    "\n",
    "    #textDict = dict(wordDictionary)\n",
    "\n",
    "    #for word in textDict.keys():\n",
    "        #if word in word_counts:\n",
    "            #word_counts[word] += 1\n",
    "        #else:\n",
    "            #word_counts[word] = 1\n",
    "\n",
    "    #return wordDictionary\n",
    "\n",
    "# Call wordEnumeration funtion on all rows, using wordCounts since it's already a dictionary with the unique words\n",
    "#main_df = main_df.withColumn(\"wordCounts\", pyspark.sql.functions.udf(lambda x: wordEnumeration(x))(\"wordCounts\"))\n",
    "\n",
    "#main_df.show(20)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Method 3: RDD map and reduce from assignment 4\n",
    "# Works but was slower than the main working method\n",
    "\n",
    "#word_counts = main_df.select(\"text\").rdd.flatMap(lambda line: str(line).split()) \\\n",
    "#  .map(lambda word: (word, 1)) \\\n",
    "#  .reduceByKey(lambda x, y: x+y) \\\n",
    "#  .sortByKey() \\\n",
    "#  .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BijEVoZ_4qP0"
   },
   "source": [
    "## Term Frequency / Word Counts in Each Text String\n",
    "Count how many times each term occurs in each string of text\n",
    "Store in a dictionary in a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "edPAcsva4y7W"
   },
   "outputs": [],
   "source": [
    "# Count occurrences of all words in a string; return as dictionary\n",
    "def wordCount(text):\n",
    "\n",
    "    counts = dict()\n",
    "    words = text.split()\n",
    "\n",
    "    for word in words:\n",
    "        if word in counts:\n",
    "            counts[word] += 1\n",
    "        else:\n",
    "            counts[word] = 1\n",
    "\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "ERQeoeO3vNx7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|                text|               title|               words|          wordCounts|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "| 12|  short descripti...|           Anarchism|virtues causal 18...|{half=2, nowadays...|\n",
      "| 39|  short descripti...|              Albedo|2008apopt jetcont...|{news_releases=2,...|\n",
      "|290|  short descripti...|                   A|putman unchanged ...|{exception=1, ࠀ=1...|\n",
      "|303|  short descripti...|             Alabama|744 outbreak 2010...|{prepare=1, seapo...|\n",
      "|305|  short descripti...|            Achilles|virtues ἄειδε dra...|{vulci=2, fifty=1...|\n",
      "|307|  short descripti...|     Abraham Lincoln|wanted hung outbr...|{pills=3, half=2,...|\n",
      "|308|  short descripti...|           Aristotle|virtues causal 20...|{minerals=1, half...|\n",
      "|309|  short descripti...|An American in Paris|10154 citations i...|{germinal=1, germ...|\n",
      "|316|  short descripti...|Academy Award for...|kim motsumoto cre...|{geisha=2, hamlet...|\n",
      "|324|  short descripti...|      Academy Awards|tabs contest kim ...|{half=3, surgeon=...|\n",
      "|330|  use dmy dates d...|             Actrius|drama called a ke...|{01=4, prepare=1,...|\n",
      "|332|  short descripti...|     Animalia (book)|isbn sites words ...|{been=1, referenc...|\n",
      "|334|  short descripti...|International Ato...|isbn phase called...|{ticking=1, slowe...|\n",
      "|336|  short descripti...|            Altruism|virtues donation ...|{german=1, functi...|\n",
      "|339|  short descripti...|            Ayn Rand|contest wanted dr...|{allison=2, vasil...|\n",
      "|340|  short descripti...|        Alain Connes|isbn 1976 sites t...|{references=1, ye...|\n",
      "|344|  short descripti...|          Allan Dwan|isbn trade eagan ...|{half=2, referenc...|\n",
      "|358|  short descripti...|             Algeria|14118852 citation...|{population_censu...|\n",
      "|359|  short descripti...|List of Atlas Shr...|lessons problem b...|{willers=6, withd...|\n",
      "|569|  short descripti...|        Anthropology|1898 wanted creek...|{half=1, upload=3...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Elapsed time: 0:00:00.84\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Add column to main df with counts of all unique words in the text (lazily evaluated)\n",
    "main_df = main_df.withColumn(\"wordCounts\", pyspark.sql.functions.udf(lambda x: wordCount(x))(\"text\"))\n",
    "\n",
    "# Call show to actually process the change\n",
    "main_df.show(20)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m28_iENikFXU"
   },
   "source": [
    "## TF/IDF Weights\n",
    "\n",
    "Reduce word count values by their IDF. This makes common words less valuable in a search query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Jm-lEkYDk0sM"
   },
   "outputs": [],
   "source": [
    "# Write function to reduce dictionary values by IDF weights\n",
    "# Use it for both the lambda function (indexer) and query processing (ranker)\n",
    "def TDIDFWeights(text, isString):\n",
    "\n",
    "    # Convert string to dictionary if necessary\n",
    "    if isString:\n",
    "      textDict = dict(text)\n",
    "    else:\n",
    "      textDict = text\n",
    "\n",
    "    # Reduce weights; don't try to access non-existent dictionary entries\n",
    "    for i in textDict.keys():\n",
    "      try:\n",
    "        textDict[i] = float(textDict[i] / word_counts[i])\n",
    "      except:\n",
    "        continue\n",
    "\n",
    "    return textDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "Gal8mod-FaqR"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| id|                text|               title|               words|          wordCounts|   reducedWordCounts|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| 12|  short descripti...|           Anarchism|virtues causal 18...|{half=2, nowadays...|{half=0.05, nowad...|\n",
      "| 39|  short descripti...|              Albedo|2008apopt jetcont...|{news_releases=2,...|{news_releases=1....|\n",
      "|290|  short descripti...|                   A|putman unchanged ...|{exception=1, ࠀ=1...|{exception=0.0434...|\n",
      "|303|  short descripti...|             Alabama|744 outbreak 2010...|{prepare=1, seapo...|{prepare=0.1, sea...|\n",
      "|305|  short descripti...|            Achilles|virtues ἄειδε dra...|{vulci=2, fifty=1...|{vulci=2.0, fifty...|\n",
      "|307|  short descripti...|     Abraham Lincoln|wanted hung outbr...|{pills=3, half=2,...|{pills=3.0, half=...|\n",
      "|308|  short descripti...|           Aristotle|virtues causal 20...|{minerals=1, half...|{minerals=0.09090...|\n",
      "|309|  short descripti...|An American in Paris|10154 citations i...|{germinal=1, germ...|{germinal=1.0, ge...|\n",
      "|316|  short descripti...|Academy Award for...|kim motsumoto cre...|{geisha=2, hamlet...|{geisha=2.0, haml...|\n",
      "|324|  short descripti...|      Academy Awards|tabs contest kim ...|{half=3, surgeon=...|{half=0.075, surg...|\n",
      "|330|  use dmy dates d...|             Actrius|drama called a ke...|{01=4, prepare=1,...|{01=0.0625, prepa...|\n",
      "|332|  short descripti...|     Animalia (book)|isbn sites words ...|{been=1, referenc...|{been=0.012048192...|\n",
      "|334|  short descripti...|International Ato...|isbn phase called...|{ticking=1, slowe...|{ticking=0.5, slo...|\n",
      "|336|  short descripti...|            Altruism|virtues donation ...|{german=1, functi...|{german=0.0285714...|\n",
      "|339|  short descripti...|            Ayn Rand|contest wanted dr...|{allison=2, vasil...|{allison=0.5, vas...|\n",
      "|340|  short descripti...|        Alain Connes|isbn 1976 sites t...|{references=1, ye...|{references=0.011...|\n",
      "|344|  short descripti...|          Allan Dwan|isbn trade eagan ...|{half=2, referenc...|{half=0.05, refer...|\n",
      "|358|  short descripti...|             Algeria|14118852 citation...|{population_censu...|{population_censu...|\n",
      "|359|  short descripti...|List of Atlas Shr...|lessons problem b...|{willers=6, withd...|{willers=6.0, wit...|\n",
      "|569|  short descripti...|        Anthropology|1898 wanted creek...|{half=1, upload=3...|{half=0.025, uplo...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Elapsed time: 0:00:01.03\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Add column to main df with dictionary containing all unique words reduced by IDF weights\n",
    "main_df = main_df.withColumn(\"reducedWordCounts\", pyspark.sql.functions.udf(lambda x: TDIDFWeights(x, True))(\"wordCounts\"))\n",
    "\n",
    "# Call show to actually process the change\n",
    "main_df.show(20)\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSV0c9IFHSJ4"
   },
   "source": [
    "# Ranking Engine\n",
    "Run a text query against the indexed documents to find the most relevant ones\n",
    "A new search can be run by changing the text query and running the ranking engine again (no need to rerun the indexing engine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S4VC2Q_jRAxo"
   },
   "source": [
    "## Text Query\n",
    "The search query to be run against the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0lWrnfY9RH64"
   },
   "outputs": [],
   "source": [
    "text_query = \"abraham lincoln\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mhJ1vumYRso2"
   },
   "source": [
    "## Query Vectorizer\n",
    "Creates a dictionary containing all words in the query and their counts, then reduces it by TD/IDF weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "03syDDWgRzS8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'abraham': 0.1111111111111111, 'lincoln': 0.1}\n"
     ]
    }
   ],
   "source": [
    "vectorized_text_query = TDIDFWeights(wordCount(textCleanup(text_query)), False)\n",
    "print(vectorized_text_query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "etqp0XyxTZQw"
   },
   "source": [
    "## Relevance Analysis\n",
    "Runs the text query against each document to calculate a relevance number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "w-0GdvJpU5nt"
   },
   "outputs": [],
   "source": [
    "# Write function to calculate the relevance number between a text query and a document\n",
    "def calculateRelevance(text):\n",
    "\n",
    "    textDict = dict(text)\n",
    "    relevanceNumber = float(0)\n",
    "\n",
    "    for i in vectorized_text_query.keys():\n",
    "      if i in textDict.keys():\n",
    "        relevanceNumber += vectorized_text_query[i] * textDict[i]\n",
    "\n",
    "    return relevanceNumber"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "Sgvfm544VD33"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
      "| id|                text|               title|               words|          wordCounts|   reducedWordCounts|relevanceNumber|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
      "| 12|  short descripti...|           Anarchism|virtues causal 18...|{half=2, nowadays...|{half=0.05, nowad...|            0.0|\n",
      "| 39|  short descripti...|              Albedo|2008apopt jetcont...|{news_releases=2,...|{news_releases=1....|            0.0|\n",
      "|290|  short descripti...|                   A|putman unchanged ...|{exception=1, ࠀ=1...|{exception=0.0434...|            0.0|\n",
      "|303|  short descripti...|             Alabama|744 outbreak 2010...|{prepare=1, seapo...|{prepare=0.1, sea...|            0.0|\n",
      "|305|  short descripti...|            Achilles|virtues ἄειδε dra...|{vulci=2, fifty=1...|{vulci=2.0, fifty...|            0.0|\n",
      "|307|  short descripti...|     Abraham Lincoln|wanted hung outbr...|{pills=3, half=2,...|{pills=3.0, half=...|      10.916049|\n",
      "|308|  short descripti...|           Aristotle|virtues causal 20...|{minerals=1, half...|{minerals=0.09090...|            0.0|\n",
      "|309|  short descripti...|An American in Paris|10154 citations i...|{germinal=1, germ...|{germinal=1.0, ge...|            0.0|\n",
      "|316|  short descripti...|Academy Award for...|kim motsumoto cre...|{geisha=2, hamlet...|{geisha=2.0, haml...|           0.02|\n",
      "|324|  short descripti...|      Academy Awards|tabs contest kim ...|{half=3, surgeon=...|{half=0.075, surg...|           0.04|\n",
      "|330|  use dmy dates d...|             Actrius|drama called a ke...|{01=4, prepare=1,...|{01=0.0625, prepa...|            0.0|\n",
      "|332|  short descripti...|     Animalia (book)|isbn sites words ...|{been=1, referenc...|{been=0.012048192...|            0.0|\n",
      "|334|  short descripti...|International Ato...|isbn phase called...|{ticking=1, slowe...|{ticking=0.5, slo...|            0.0|\n",
      "|336|  short descripti...|            Altruism|virtues donation ...|{german=1, functi...|{german=0.0285714...|    0.037037037|\n",
      "|339|  short descripti...|            Ayn Rand|contest wanted dr...|{allison=2, vasil...|{allison=0.5, vas...|            0.0|\n",
      "|340|  short descripti...|        Alain Connes|isbn 1976 sites t...|{references=1, ye...|{references=0.011...|            0.0|\n",
      "|344|  short descripti...|          Allan Dwan|isbn trade eagan ...|{half=2, referenc...|{half=0.05, refer...|            0.0|\n",
      "|358|  short descripti...|             Algeria|14118852 citation...|{population_censu...|{population_censu...|            0.0|\n",
      "|359|  short descripti...|List of Atlas Shr...|lessons problem b...|{willers=6, withd...|{willers=6.0, wit...|            0.0|\n",
      "|569|  short descripti...|        Anthropology|1898 wanted creek...|{half=1, upload=3...|{half=0.025, uplo...|           0.01|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+---------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Elapsed time: 0:00:01.51\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Add column to main df with relevance number for each document\n",
    "main_df = main_df.withColumn(\"relevanceNumber\", pyspark.sql.functions.udf(lambda x: calculateRelevance(x))(\"reducedWordCounts\"))\n",
    "main_df = main_df.withColumn(\"relevanceNumber\", main_df[\"relevanceNumber\"].cast('float'))\n",
    "\n",
    "# Call show to actually process the change\n",
    "main_df.show()\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ruFfpeQPVtYV"
   },
   "source": [
    "## Ranker/Output\n",
    "Write results to file and read into Pandas\n",
    "Determine the top ten most relevant documents and print the corresponding links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6NCNzUYACXMT"
   },
   "source": [
    "## Output\n",
    "Output the ID, title, link, and relevance number of the top ten most relevant documents to the text query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "MYlmyNxT2Qn4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:01.37\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Still doesn't work for large datasets\n",
    "# Write to a file\n",
    "# Rename columns\n",
    "# Generate Link column\n",
    "# https://sparkbyexamples.com/pyspark/pyspark-write-dataframe-to-csv-file/?expand_article=1\n",
    "main_df \\\n",
    "  .select(\"id\", \"title\", \"relevanceNumber\") \\\n",
    "  .withColumnRenamed(\"id\",\"ID\").withColumnRenamed(\"title\",\"Title\").withColumnRenamed(\"relevanceNumber\",\"Relevance Number\") \\\n",
    "  .withColumn(\"Link\", pyspark.sql.functions.udf(lambda x: \"https://en.wikipedia.org/wiki/\" + x.replace(' ', '_') + \" \")(\"Title\")) \\\n",
    "  .write.options(header='True', delimiter=',') \\\n",
    "  .mode('overwrite') \\\n",
    "  .csv(\"C:\\\\Users\\\\Brian\\\\Downloads\\\\results.csv\")\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Title</th>\n",
       "      <th>Relevance Number</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>307</td>\n",
       "      <td>Abraham Lincoln</td>\n",
       "      <td>10.916049</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abraham_Lincoln</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>736</td>\n",
       "      <td>Albert Einstein</td>\n",
       "      <td>0.067037</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Albert_Einstein</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>324</td>\n",
       "      <td>Academy Awards</td>\n",
       "      <td>0.040000</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Academy_Awards</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>336</td>\n",
       "      <td>Altruism</td>\n",
       "      <td>0.037037</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Altruism</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>711</td>\n",
       "      <td>Albert Sidney Johnston</td>\n",
       "      <td>0.032346</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Albert_Sidney_Jo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>662</td>\n",
       "      <td>Apollo 11</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Apollo_11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>678</td>\n",
       "      <td>Abel</td>\n",
       "      <td>0.024691</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Abel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>316</td>\n",
       "      <td>Academy Award for Best Production Design</td>\n",
       "      <td>0.020000</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Academy_Award_fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>657</td>\n",
       "      <td>Bitumen</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Bitumen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>700</td>\n",
       "      <td>Arthur Schopenhauer</td>\n",
       "      <td>0.012346</td>\n",
       "      <td>https://en.wikipedia.org/wiki/Arthur_Schopenhauer</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     ID                                     Title  Relevance Number  \\\n",
       "5   307                           Abraham Lincoln         10.916049   \n",
       "91  736                           Albert Einstein          0.067037   \n",
       "9   324                            Academy Awards          0.040000   \n",
       "13  336                                  Altruism          0.037037   \n",
       "86  711                    Albert Sidney Johnston          0.032346   \n",
       "56  662                                 Apollo 11          0.024691   \n",
       "67  678                                      Abel          0.024691   \n",
       "8   316  Academy Award for Best Production Design          0.020000   \n",
       "53  657                                   Bitumen          0.012346   \n",
       "78  700                       Arthur Schopenhauer          0.012346   \n",
       "\n",
       "                                                 Link  \n",
       "5       https://en.wikipedia.org/wiki/Abraham_Lincoln  \n",
       "91      https://en.wikipedia.org/wiki/Albert_Einstein  \n",
       "9        https://en.wikipedia.org/wiki/Academy_Awards  \n",
       "13             https://en.wikipedia.org/wiki/Altruism  \n",
       "86  https://en.wikipedia.org/wiki/Albert_Sidney_Jo...  \n",
       "56            https://en.wikipedia.org/wiki/Apollo_11  \n",
       "67                 https://en.wikipedia.org/wiki/Abel  \n",
       "8   https://en.wikipedia.org/wiki/Academy_Award_fo...  \n",
       "53              https://en.wikipedia.org/wiki/Bitumen  \n",
       "78  https://en.wikipedia.org/wiki/Arthur_Schopenhauer  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed time: 0:00:00.01\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "#https://sparkbyexamples.com/pandas/pandas-read-multiple-csv-files/?expand_article=1\n",
    "# Get CSV files list from a folder\n",
    "path = r'C:\\Users\\Brian\\Downloads\\results.csv'\n",
    "csv_files = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "# Read each CSV file into DataFrame\n",
    "# This creates a list of dataframes\n",
    "df_list = (pd.read_csv(file) for file in csv_files)\n",
    "\n",
    "# Concatenate all DataFrames\n",
    "results   = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "# Select top 10 results and print\n",
    "display(results.nlargest(10, \"Relevance Number\"))\n",
    "\n",
    "elapsed_time = time.time() - start_time\n",
    "print(\"Elapsed time: {}\".format(hms_string(elapsed_time)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gw2ehVM1zz0p",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Alternative attempts at the Ranker / Output step\n",
    "These methods were either slower or didn't end up working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DnB-NZPLVyS1"
   },
   "outputs": [],
   "source": [
    "# Method 1: Not working\n",
    "#main_df.select(f.max(f.col(\"relevanceNumber\")).alias(\"MAX\")).limit(10).collect()[0].MAX\n",
    "\n",
    "\n",
    "# Method 2: Pandas method; works but takes about the same amount of time as Spark SQL\n",
    "#pandas_main_df = main_df.select(\"id\", \"title\", \"relevanceNumber\").toPandas()#['title']\n",
    "#pandas_main_df = pandas_main_df.nlargest(10, \"relevanceNumber\")#sort_values('relevanceNumber', ascending = False)#.groupby('relevanceNumber').\n",
    "#print (pandas_main_df)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Method 3: Data frame .sort\n",
    "# Works but is much slower than Spark SQL\n",
    "#results_df = main_df.sort(\"relevanceNumber\", ascending=False).show(10)\n",
    "#results_df = results_df.withColumnRenamed(\"id\",\"ID\").withColumnRenamed(\"title\",\"Title\").withColumnRenamed(\"relevanceNumber\",\"Relevance Number\")\n",
    "#results_df = results_df.withColumn(\"Link\", pyspark.sql.functions.udf(lambda x: \"https://en.wikipedia.org/wiki/\" + x.replace(' ', '_') + \" \")(\"Title\"))\n",
    "#results_df.select(\"ID\", \"Title\", \"Link\", \"Relevance Number\").show(10, False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Method 4: Spark SQL method\n",
    "# Works well on smaller datasets but has memory issues when they get too big\n",
    "# Does not account for extremely small numbers (treats 9.0E-7 as higher than 1 since it begins with 9)\n",
    "#main_df.createOrReplaceTempView(\"main\")\n",
    "#results_df = spark.sql(\"SELECT id, title, relevanceNumber FROM main ORDER BY relevanceNumber DESC LIMIT 10\").rdd.map(list)\n",
    "\n",
    "# Rename column names\n",
    "#results_df = results_df.toDF().withColumnRenamed(\"_1\",\"ID\").withColumnRenamed(\"_2\",\"Title\").withColumnRenamed(\"_3\",\"Relevance Number\")\n",
    "\n",
    "# Add link row; uses title with spaces replaced by underscores; add a space to the end so it plays nice with output tables\n",
    "#results_df = results_df.withColumn(\"Link\", pyspark.sql.functions.udf(lambda x: \"https://en.wikipedia.org/wiki/\" + x.replace(' ', '_') + \" \")(\"Title\"))\n",
    "\n",
    "# Convert pipelinedRDD to dataframe and output\n",
    "#results_df.show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ace77c0e-3aaa-4060-abdf-7332b0f36ef5"
   },
   "source": [
    "# Close Session\n",
    "Close the Spark session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f964cc44-b9ee-4982-ba6b-e15bbc4343e3"
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jlQTy0mzxrYv",
    "gw2ehVM1zz0p"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
